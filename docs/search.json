[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Introduction\nBased on our EDA, we can expect to have some reasonably strong predictors selected for our models. We will split our data into 70% training and 30% testing data then create 3 different types of models:\n\nLogistic Regression Models\nClassification Tree Models\nRandom Forest Models\n\nWe will use logLoss as our metric to evaluate models and do 5 folt cross-validation to slect the best model from each type of model. At the very end, we will compare our best model from each of the Model types and have an overall/final best model.\nLet’s begin by loading our packages and splitting our data.\n\n# Load packages\nlibrary(caret)\nlibrary(dplyr)\nlibrary(glmnet)  \nlibrary(MLmetrics)\nlibrary(rpart)\nlibrary(randomForest)\n\n# Read the data in like we did for EDA\ndiabetes_data &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Pick response and variables to be investigated and mutate them appropriately\ndiabetes_data &lt;- diabetes_data[, c(\"Diabetes_binary\", \"BMI\", \"PhysActivity\", \"HighBP\")] %&gt;%\n    mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n  )\n\n# Split the data\nset.seed(1)\ntrain_index &lt;- createDataPartition(diabetes_data$Diabetes_binary, p = 0.7, list = FALSE)\ntrain_df &lt;- diabetes_data[train_index, ]\ntest_df  &lt;- diabetes_data[-train_index, ]\n\n\n\nLogistic Regression Models\nLogistic Regression is a statistical model used for binary classification problems (where the outcome is one of two categories). This model predicts the probability that a given observation belongs to a particular class. For our context, we are predicting the probability that a given observation belongs to the following 2 categories:\n-Yes: Diabetes\n-No: No Diabetes.\n\n# 1. Set up trainControl with logLoss\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  classProbs = TRUE,\n  summaryFunction = mnLogLoss\n)\n\n# Candidate Model 1: Only BMI\nset.seed(1) # reset the seed for each model to ensure the same folds\nlogmodel1 &lt;- train(\n  Diabetes_binary ~ BMI,\n  data = train_df,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = ctrl,\n  metric = \"logLoss\"\n)\n\n# Candidate Model 2: BMI + Physical Activity\nset.seed(1)\nlogmodel2 &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity,\n  data = train_df,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = ctrl,\n  metric = \"logLoss\"\n)\n\n# Candidate Model 3: BMI + Physical Activity + High Blood Pressure\nset.seed(1)\nlogmodel3 &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity + HighBP,\n  data = train_df,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = ctrl,\n  metric = \"logLoss\"\n)\n\n# Compare models\nlogresults &lt;- data.frame(\n  Model = c(\"BMI Only\", \"BMI + PhysActivity\", \"BMI + PhysActivity + HighBP\"),\n  LogLoss = c(\n    min(logmodel1$results$logLoss),\n    min(logmodel2$results$logLoss),\n    min(logmodel3$results$logLoss)\n  )\n)\n\nprint(logresults)\n\n                        Model   LogLoss\n1                    BMI Only 0.3839255\n2          BMI + PhysActivity 0.3803672\n3 BMI + PhysActivity + HighBP 0.3549494\n\n\nlogmodel3 has the smallest logLoss, indicating it is our best logistics regression model.\n\n\nClassification Tree\nA classification tree is a type of decision tree used for predicting categorical outcomes. In our case, the goal is to predict whether an individual has diabetes (Yes or No) based on characteristics like BMI, physical activity, and blood pressure.\nA classification tree works by recursively partitioning the dataset into smaller and smaller groups that are increasingly “pure” — meaning they contain mostly one class (e.g., mostly diabetics or mostly non-diabetics).\n\n# Create grid\ncp_grid &lt;- expand.grid(cp = c(0.00001, 0.0001, 0.000001)) #small cp values are required for any model differences to be present\n\n# Model 1: BMI only\nset.seed(1)\ntree1 &lt;- train(\n  Diabetes_binary ~ BMI,\n  data = train_df,\n  method = \"rpart\",\n  trControl = ctrl,\n  metric = \"logLoss\",\n  tuneGrid = cp_grid\n)\n\n# Model 2: BMI + Physical Activity\nset.seed(1)\ntree2 &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity,\n  data = train_df,\n  method = \"rpart\",\n  trControl = ctrl,\n  metric = \"logLoss\",\n  tuneGrid = cp_grid\n)\n\n# Model 3: BMI + Physical Activity + HighBP\nset.seed(1)\ntree3 &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity + HighBP,\n  data = train_df,\n  method = \"rpart\",\n  trControl = ctrl,\n  metric = \"logLoss\",\n  tuneGrid = cp_grid\n)\n\ntreeresults &lt;- data.frame(\n  Model = c(\"BMI Only\", \"BMI + PhysActivity\", \"BMI + PhysActivity + HighBP\"),\n  LogLoss = c(\n    min(tree1$results$logLoss),\n    min(tree2$results$logLoss),\n    min(tree3$results$logLoss)\n  )\n)\n\nprint(treeresults)\n\n                        Model   LogLoss\n1                    BMI Only 0.3842547\n2          BMI + PhysActivity 0.3837222\n3 BMI + PhysActivity + HighBP 0.3561990\n\n\nOur best classification tree model is tree3 since it has the lowest logLoss. Our logLoss values are similar to the logLoss values from the logistic models!\n\n\nRandom Forest\nA random forest is an ensemble machine learning method that builds a collection (or “forest”) of decision trees and combines their predictions to improve performance and reduce overfitting. It is particularly effective for classification tasks like predicting whether an individual has diabetes.\n\n# Model 1: BMI only\nset.seed(1)\nrf1 &lt;- train(\n  Diabetes_binary ~ BMI,\n  data = train_df,\n  method = \"rf\",\n  trControl = ctrl,\n  metric = \"logLoss\",\n  tuneGrid = expand.grid(mtry = 1),\n  ntree = 100\n)\n\n# Model 2: BMI + Physical Activity\nset.seed(1)\nrf2 &lt;- train(\n  Diabetes_binary ~ BMI + PhysActivity,\n  data = train_df,\n  method = \"rf\",\n  trControl = ctrl,\n  metric = \"logLoss\",\n  tuneGrid = expand.grid(mtry = c(1, 2)),\n  ntree = 100\n)\n\n# Model 3: BMI + Physical Activity + HighBP\nset.seed(1)\nrf3 &lt;- train(\n  Diabetes_binary ~ .,\n  data = train_df,\n  method = \"rf\",\n  trControl = ctrl,\n  metric = \"logLoss\",\n  tuneGrid = expand.grid(mtry = c(1, 2, 3)),\n  ntree = 100\n)\n\n# results\nrfresults &lt;- data.frame(\n  Model = c(\"BMI Only\", \"BMI + PhysActivity\", \"BMI + PhysActivity + HighBP\"),\n  LogLoss = c(\n    min(rf1$results$logLoss),\n    min(rf2$results$logLoss),\n    min(rf3$results$logLoss)\n  )\n)\n\nprint(rfresults)\n\n                        Model  LogLoss\n1                    BMI Only 4.753101\n2          BMI + PhysActivity 4.684810\n3 BMI + PhysActivity + HighBP 4.448119\n\n\n\n\nFinal Model Selection\nNow that we have every model fit, lets compare the best models from each on the test data, and pick an overall best model!\n\n# Use Test data\nprobs_lr &lt;- predict(logmodel3, newdata = test_df, type = \"prob\")[, \"Yes\"]\nprobs_tree &lt;- predict(tree3, newdata = test_df, type = \"prob\")[, \"Yes\"]\nprobs_rf &lt;- predict(rf3, newdata = test_df, type = \"prob\")[, \"Yes\"]\n\n# Calculate logLoss\nactual &lt;- ifelse(test_df$Diabetes_binary == \"Yes\", 1, 0)\nlogloss_lr &lt;- LogLoss(probs_lr, actual)\nlogloss_tree &lt;- LogLoss(probs_tree, actual)\nlogloss_rf &lt;- LogLoss(probs_rf, actual)\n\n# Compare results\nfinalresults &lt;- data.frame(\n  Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(logloss_lr, logloss_tree, logloss_rf)\n)\n\nprint(finalresults)\n\n                Model   LogLoss\n1 Logistic Regression 0.3542397\n2 Classification Tree 0.3551220\n3       Random Forest 4.5110943\n\n\nOur overall best model is our 3rd logistics regression model. It just barely out-performed the classification tree model. The random forest model seems to be misfit in some way, so it was not even close to the other two models."
  },
  {
    "objectID": "Modeling.html#quarto",
    "href": "Modeling.html#quarto",
    "title": "Modeling",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Modeling.html#running-code",
    "href": "Modeling.html#running-code",
    "title": "Modeling",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Introduction\nThis project uses the Diabetes Health Indicators, which includes over 250,000 observations from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). Each observation represents an individual and includes a variety of health-related variables:\n\nDiabetes_binary: Indicates whether the individual has been diagnosed with diabetes (our response variable)\nBMI: Body Mass Index\nPhysActivity: Whether the individual has engaged in physical activity in the last 30 days (not including job)\nHighBP: Whether the individual has high blood pressure\nThere are 18 more variables, but they will be excluded from this project for simplicity.\n\nThe goal of this exploratory data analysis (EDA) is to better understand the distribution and relationships between key health indicators and diabetes status. This includes examining individual variable distributions and how each potential predictor relates to the outcome.\nOur ultimate objective is to develop a model that can reasonably predict whether an individual is likely to have diabetes based on these health indicators. By identifying meaningful patterns and associations during EDA, we aim to inform our modeling decisions and improve predictive accuracy. We will create 3 different types of models and select the best model from each model type. After that, we will compare these models and pick the overall best model.\nOur response variable will be Diabetes_binary and the 3 selected predictors are: BMI, PhysActivity, and HighBP. Studies have shown strong links between these variables and diabetes outcomes. For example, this article by Mokdad et al. (2005) published in Diabetes Care highlights how obesity and inactivity contribute significantly to the prevalence of diabetes. This article also mentions that blood pressure is connected to diabetes.\n\n\nData\n\n# Load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\n\n# Read in the data\ndiabetes_data &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Pick response and variables to be investigated and mutate them appropriately\ndiabetes_data &lt;- diabetes_data[, c(\"Diabetes_binary\", \"BMI\", \"PhysActivity\", \"HighBP\")] %&gt;%\n    mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n  )\nhead(diabetes_data)\n\n  Diabetes_binary BMI PhysActivity HighBP\n1              No  40           No    Yes\n2              No  25          Yes     No\n3              No  28           No    Yes\n4              No  27          Yes    Yes\n5              No  24          Yes    Yes\n6              No  25          Yes    Yes\n\n#check for missing values\nany(is.na(diabetes_data))\n\n[1] FALSE\n\n\nExcellent! There are no missing values and our variable classifications make sense!\n\n\nSummarizations\nFirst, let’s just produce some basic statistics about each variable.\n\nsummary(diabetes_data)\n\n Diabetes_binary      BMI        PhysActivity HighBP      \n No :218334      Min.   :12.00   No : 61760   No :144851  \n Yes: 35346      1st Qu.:24.00   Yes:191920   Yes:108829  \n                 Median :27.00                            \n                 Mean   :28.38                            \n                 3rd Qu.:31.00                            \n                 Max.   :98.00                            \n\n\nWe can see that around 15% of the individuals in this study have diabetes. This should be a sufficient proportion to allow for meaningful predictions.\nFor BMI, our mean is 1 point higher than our median, suggesting a slight right skew. We will look into this further. Considering the Normal range for BMI is 18.5-25, our mean is above normal (28.38) and our 1st quartile is barely within normal at 24. This suggest that our population is largely over weight, which aligns with national statistics from the US.\nPhysical activity is consistent with expectations: about one-fourth of participants reported no physical activity in the last 30 days. High blood pressure is prevalent, with over 40% of the sample having it.\nNow, let’s look at the distribution of BMI.\n\n# Histogram of BMI\nggplot(diabetes_data, aes(x = BMI)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\") +\n  labs(title = \"Distribution of BMI\")\n\n\n\n\n\n\n\n\nWe can now confirm that our BMI data has a right skew. While extreme BMI values (above 50) do exist, they are relatively rare. This supports the idea that the dataset is broadly representative but includes some outliers.\nNow, let’s plot physical activity.\n\n# Bar plot of PhysActivity\nggplot(diabetes_data, aes(x = PhysActivity)) +\n  geom_bar(fill = \"darkgreen\") +\n  labs(title = \"Physical Activity Distribution\", x = \"Physical Activity in the Last 30 Days\")\n\n\n\n\n\n\n\n\nThe physical activity bar plot shows that roughly 75% of the population is active.\nLet’s briefly plot and look at high blood pressure.\n\nggplot(diabetes_data, aes(x = HighBP, fill = HighBP)) +\n  geom_bar() +\n  labs(\n    title = \"High Blood Pressure Distribution\",\n    x = \"High Blood Pressure\",\n    y = \"Count\"\n  ) +\n  scale_fill_manual(values = c(\"No\" = \"steelblue\", \"Yes\" = \"firebrick\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow we have a visual representation of high blood pressure counts. Again, we can see that just under half of our sample has high blood pressure.\nNow let’s make a boxplot for BMI and diabetes.\n\n# BMI vs. Diabetes\nggplot(diabetes_data, aes(x = Diabetes_binary, y = BMI)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"BMI by Diabetes Status\", x = \"Diabetes Status\", y = \"BMI\")\n\n\n\n\n\n\n\n\nWe can see that the diabetes quartiles have slightly higher BMI. Maybe we can see this relationship better with a different plot.\nLet’s bin BMI as typical ranges: Underweight (less than 18.5), Normal (18.5-25), Overweight (25-3), Obese (30+). I added counts for each bin since we know that our BMI data is not evenly distributed into each bin.\n\n# Create bins for classification ranges\ndiabetes_data &lt;- diabetes_data %&gt;%\n  mutate(BMI_category = case_when(\n    BMI &lt; 18.5 ~ \"Underweight\",\n    BMI &gt;= 18.5 & BMI &lt; 25 ~ \"Normal\",\n    BMI &gt;= 25 & BMI &lt; 30 ~ \"Overweight\",\n    BMI &gt;= 30 ~ \"Obese\"\n  )) %&gt;%\n  mutate(BMI_category = factor(BMI_category, levels = c(\"Underweight\", \"Normal\", \"Overweight\", \"Obese\")))\n\n# Get total counts per BMI category\nbmi_counts &lt;- diabetes_data %&gt;%\n  count(BMI_category) %&gt;%\n  mutate(label_y = 1.05)\n\n# Plot with proportion bars and count labels\nggplot(diabetes_data, aes(x = BMI_category, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  geom_text(\n    data = bmi_counts,\n    aes(x = BMI_category, y = label_y, label = paste0(\"n = \", n)),\n    inherit.aes = FALSE\n  ) +\n  scale_y_continuous(labels = percent_format(), limits = c(0, 1.1)) +\n  labs(\n    title = \"Diabetes Rate by BMI Category\",\n    x = \"BMI Category\",\n    y = \"Proportion\",\n    fill = \"Diabetes\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the above plot we can now see a clear increase in proportion with Diabetes for Overweight individuals and an even greater increase for Obese individuals. This should be a strong predictor in our models.\nLet’s look at physical activity in relation to diabetes to see if it might also be a strong predictor.\n\n# PhysActivity vs. Diabetes\nggplot(diabetes_data, aes(x = PhysActivity, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Diabetes Rate by Physical Activity\", x = \"Physical Activity\", y = \"Proportion\", fill = \"Diabetes\")\n\n\n\n\n\n\n\n\nWe observe a noticeably higher proportion of individuals without recent physical activity who have diabetes, compared to those who are physically active. Since the proportion of diabetes cases differs between the two groups, this suggests that physical activity may have predictive value in determining diabetes status.\nNow let’s look at the relationship between diabetes and high blood pressure.\n\n# HighBP vs. Diabetes\nggplot(diabetes_data, aes(x = HighBP, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Diabetes Rate by High Blood Pressure\", x = \"High Blood Pressure\", y = \"Proportion\", fill = \"Diabetes\")\n\n\n\n\n\n\n\n\nThese proportions are extremely different, suggesting high blood pressure may be a strong predictor for diabetes.\nNow, let’s move on to modeling to see just how strong of predictors our variables are for diabetes. Click here for the Modeling Page"
  }
]